{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron de Camada Única\n",
    "\n",
    "Do inglês Singles Layer Perceptron (SLP), é uma das primeiras e mais antigas redes neurais criada. Foi proposta por Frank Rosenblatt em 1958. Por ser o tipo mais imples de rede neural, ela apenas classifica casos linearmente separáveis de forma binária. \n",
    "\n",
    "Uma SLP é uma rede de alimentação direta baseada em uma função de transferência. Essas funções são equações matemáticas que determinam a saída de uma rede neural.\n",
    "\n",
    "Componentes do Perceptron:\n",
    "- Input: entradas reais ou binárias;\n",
    "- Peso: cada entrada é associada à um peso, representando o seu grau de importância;\n",
    "- Bias: esse termo permite o neurônio ser ativado até quando a entrada é zero, mitigando o problema de _vanishing gradient_;\n",
    "- Função de ativação: o perceptron usa uma função degrau para determinar se a soma do produto da entrada pelo peso, somado ao bias, está acima ou abaixo de um certo limite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinamento(X_train, y_train, qtd_atributos, tx_aprendizagem, epocas, seed):\n",
    "    \"\"\"\n",
    "        Argumentos:\n",
    "        X_train -- amostras de treino\n",
    "        y_train -- resultados desejados das amostras de treino\n",
    "        qtd_atributos -- quantidade de features\n",
    "        tx_aprendizagem -- taxa de aprendizado da rede\n",
    "        epocas -- cada apresentação completa de todas as amostras pertencentes ao subconjunto de treinamento\n",
    "        seed -- para garantir que sempre será utilizado o mesmo número aleatório\n",
    "        \n",
    "        Saída:\n",
    "        parâmetros -- python dict:\n",
    "                        peso aprendido -- pesos aprendidos ao final do treinamento\n",
    "                        epocas executadas -- quantidade total de épocas ao final do treinamento\n",
    "                        evolucao do erro -- lista contendo a evolução do erro ao longo das épocas\n",
    "                        acuracia -- relação entre o total de acertos e o total de amostras\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    pesos = np.random.uniform(low=-0.5, high=0.5, size=((qtd_atributos + 1),))  # vetor de peso (atributos + bias)\n",
    "    eta = tx_aprendizagem                                                       # taxa de aprendizagem\n",
    "    epoca = 0                                                                   # contador de epoca\n",
    "    evolucao_erro = []\n",
    "\n",
    "    while epoca < epocas:\n",
    "        erro = 0\n",
    "        acertos = 0\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            x = np.append(1, X_train[i])    # adiciona o bias na amostra X_train\n",
    "            y_desejado = y_train[i]\n",
    "\n",
    "            u = np.dot(pesos, x)                                # potencial de ativação (produto escalar)\n",
    "\n",
    "            if u >= 0:                                          # função de ativação -> y_pred = sinal(u)\n",
    "                y_pred = 1\n",
    "            else:\n",
    "                y_pred = -1\n",
    "\n",
    "            if y_pred != y_desejado:                            # regra de aprendizado de Hebb\n",
    "                peso = peso + eta * (y_desejado - y_pred) * x\n",
    "                erro += 1\n",
    "            else:\n",
    "                acertos += 1\n",
    "\n",
    "            evolucao_erro.append(erro/len(X_train))     # evolução do erro ao longo das épocas\n",
    "\n",
    "        epoca += 1\n",
    "\n",
    "        if erro == 0:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"peso aprendido\": pesos,\n",
    "        \"epocas executadas\": epoca + 1,\n",
    "        \"evolucao do erro\": evolucao_erro,\n",
    "        \"acuracia\": acertos/len(X_train)\n",
    "    }\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste(X_test, y_test, peso):\n",
    "    \"\"\"\n",
    "        Argumentos:\n",
    "        X_test -- amostras de teste\n",
    "        y_test -- resultados desejados das amostras de teste\n",
    "        peso -- pesos aprendidos durante o treinamento\n",
    "        \n",
    "        Saída:\n",
    "        parâmetros -- python dict:\n",
    "                        acuracia -- relação entre o total de acertos e o total de amostras\n",
    "    \"\"\"\n",
    "\n",
    "    acertos = 0                          # contador de acertos\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        x = np.append(1, X_test[i])     # adiciona o bias na amostra X_test\n",
    "        y_desejado = y_test[i]\n",
    "\n",
    "        u = np.dot(x, peso)             # potencial de ativação com base no peso aprendido\n",
    "\n",
    "        if u >= 0:                      # função de ativação -> y_pred = sinal(u)\n",
    "            y_pred = 1\n",
    "        else:\n",
    "            y_pred = -1\n",
    "\n",
    "        if y_pred == y_desejado:\n",
    "            acertos += 1\n",
    "\n",
    "    return {\n",
    "        \"acuracia\": acertos/len(X_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando os datasets de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os datasets\n",
    "df_train_loaded = pd.read_csv(\"arquivos_csv/train_dataset1.csv\")\n",
    "df_test_loaded = pd.read_csv(\"arquivos_csv/test_dataset1.csv\")\n",
    "\n",
    "\n",
    "# Separando os dados de treinamento\n",
    "X_train = df_train_loaded.drop(\"label\", axis=1).values.T    # (n_features, n_amostras)\n",
    "y_train = df_train_loaded[\"label\"].values.reshape(1, -1)    # (1, n_amostras)\n",
    "\n",
    "# Separando os dados de teste\n",
    "X_test = df_test_loaded.drop(\"label\", axis=1).values.T\n",
    "y_test = df_test_loaded[\"label\"].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referências\n",
    "\n",
    "- Slides da aula;\n",
    "- https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
